import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

awaited_list = ['TheatreHD: –í–µ–Ω—Å–∫–∞—è –æ–ø–µ—Ä–∞. –í—Ä–µ–º–µ–Ω–∞ –≥–æ–¥–∞', '–ó–∞–∫–ª—è—Ç–∏–µ 4: –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä—è–¥', '–ö–∞—Ä—É–∑–∞',
                '–û–ø–µ—Ä–Ω—ã–π —Ñ–µ—Å—Ç–∏–≤–∞–ª—å –≤ –ú–∞—á–µ—Ä–∞—Ç–µ: –ê–∏–¥–∞']

HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

# üìÖ –î–∞—Ç—ã
start_date = datetime.today().date()
end_date = start_date + timedelta(days=365)

# üîó –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å URL
URL = f"https://afisha.me/day/film/{start_date}/{end_date}/"

# üì• –ü–æ–ª—É—á–∏—Ç—å HTML
response = requests.get(URL, headers=HEADERS)
response.raise_for_status()  # –ß—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å.
soup = BeautifulSoup(response.text, "html.parser")

# üéØ –ù–∞–π—Ç–∏ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Å—ã–ª–∫–∏
film_links = []
seen_urls = set()

for a_tag in soup.find_all("a", class_="name"):
    href = a_tag.get("href", "")
    if href.startswith("https://afisha.me/film/") and href not in seen_urls:
        title = a_tag.get_text(strip=True)
        film_links.append({
            "title": title,
            "url": href
        })
        seen_urls.add(href)

# üì§ –í—ã–≤–æ–¥
for film in film_links:
    if film['title'] in awaited_list:
        print(film)


